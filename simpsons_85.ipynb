{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e51d34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foksm\\OneDrive\\Рабочий стол\\learn_firstly\\ml_eng\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b967ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(pd.read_csv('annotation.txt'), \n",
    "                        test_size=0.1, \n",
    "                        random_state=102)\n",
    "device = torch.device('cuda')\n",
    "target_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "147d3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5706d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "data_train['character'] = enc.fit_transform(data_train['character'])\n",
    "data_test['character'] = enc.transform(data_test['character'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2b6034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['character'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27aeda3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb0183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = data_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a8da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b185e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImage():\n",
    "    def __init__(self, data, transform):\n",
    "        self.transform = transform\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data['character'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_idx = self.data.iloc[idx, -1]\n",
    "        img_path = self.data.iloc[idx, 0]\n",
    "        img_path = img_path.replace(\"characters2\", \"characters\")\n",
    "        img_path = os.path.normpath(img_path)  # Нормализуем путь для ОС\n",
    "        image_idx = Image.open(img_path)\n",
    "        image_transformed = self.transform(image_idx)\n",
    "        return image_transformed, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f4799be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomImage(data_test, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15b0356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c809980",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CustomImage(data_train, transform)\n",
    "load_train = DataLoader(dataset_train, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False)\n",
    "dataset_test = CustomImage(data_test, transform)\n",
    "load_test = DataLoader(dataset_test, \n",
    "                       batch_size=4, \n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4931c713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер батча изображений: torch.Size([32, 3, 224, 224])\n",
      "Метки: tensor([14,  1,  0,  0, 10,  0, 10,  0, 17, 16, 11, 14,  9,  7,  3,  7,  4,  3,\n",
      "        12,  0, 17, 12,  2,  3,  3,  2,  3, 10, 10,  1, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in load_train:\n",
    "    print(\"Размер батча изображений:\", images.shape)\n",
    "    print(\"Метки:\", labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79c36fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, 3)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3)\n",
    "        self.fc1 = nn.Linear(512*64, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 18)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, answer):\n",
    "        answer = F.leaky_relu(self.conv1_1(answer), 0.005)\n",
    "        answer = F.leaky_relu(self.conv1_2(answer), 0.005)\n",
    "        answer = self.maxpool(answer)\n",
    "        answer = F.leaky_relu(self.conv2_1(answer), 0.005)\n",
    "        answer = F.leaky_relu(self.conv2_2(answer), 0.005)\n",
    "        answer = self.maxpool(answer)\n",
    "        answer = F.leaky_relu(self.conv3_1(answer), 0.005)\n",
    "        answer = F.leaky_relu(self.conv3_2(answer), 0.005)\n",
    "        answer = F.leaky_relu(self.conv3_3(answer), 0.005)\n",
    "        answer = self.maxpool(answer)\n",
    "        answer = self.drop(answer)\n",
    "        answer = F.leaky_relu(self.conv4_1(answer), 0.005)\n",
    "        answer = F.leaky_relu(self.conv4_2(answer), 0.005)\n",
    "        answer = F.leaky_relu(self.conv4_3(answer), 0.005)\n",
    "        answer = self.maxpool(answer)\n",
    "        answer = answer.view(-1, 512 * 64)\n",
    "        answer = self.drop(answer)\n",
    "        answer = F.leaky_relu(self.fc1(answer), 0.005)\n",
    "        answer = F.leaky_relu(self.fc2(answer), 0.005)\n",
    "        answer = self.fc3(answer)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6fc6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)  # Оригинальный метод VGG\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f81c952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def obj(trial):\\n    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\\n    wd = trial.suggest_float('wd', 1e-5, 1e-1, log=True)\\n    drop = trial.suggest_float('drop', 1e-5, 0.5)\\n    model = CNN(dropout=drop).to(device)\\n    model.apply(init_weights)\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\\n    for epochs in range(10):\\n        epoch_loss = 0.0 \\n        for images, labels in load_train:\\n            images_cuda = images.to(device)\\n            labels_cuda = labels.to(device)\\n            optimizer.zero_grad()\\n            predict = model(images_cuda)\\n            loss = criterion(predict, labels_cuda)\\n            loss.backward()\\n            optimizer.step()\\n            epoch_loss += loss.item()\\n        trial.report(epoch_loss, epochs)\\n        if trial.should_prune():\\n            raise optuna.TrialPruned()\\n    return epoch_loss / len(load_train)\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def obj(trial):\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    wd = trial.suggest_float('wd', 1e-5, 1e-1, log=True)\n",
    "    drop = trial.suggest_float('drop', 1e-5, 0.5)\n",
    "    model = CNN(dropout=drop).to(device)\n",
    "    model.apply(init_weights)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    for epochs in range(10):\n",
    "        epoch_loss = 0.0 \n",
    "        for images, labels in load_train:\n",
    "            images_cuda = images.to(device)\n",
    "            labels_cuda = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predict = model(images_cuda)\n",
    "            loss = criterion(predict, labels_cuda)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        trial.report(epoch_loss, epochs)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    return epoch_loss / len(load_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ec55f9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"learn = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(),pruner=optuna.pruners.MedianPruner(n_startup_trials=3))\\nlearn.optimize(obj, n_trials=20)\""
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''learn = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(),pruner=optuna.pruners.MedianPruner(n_startup_trials=3))\n",
    "learn.optimize(obj, n_trials=20)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5f007393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\"Лучшие параметры:\", learn.best_params)\\nprint(\"Лучший loss:\", learn.best_value)'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(\"Лучшие параметры:\", learn.best_params)\n",
    "print(\"Лучший loss:\", learn.best_value)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c73748a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# График истории оптимизации\\noptuna.visualization.plot_optimization_history(learn)\\n\\n# Важность гиперпараметров\\noptuna.visualization.plot_param_importances(learn)\\n\\n# Зависимость loss от lr\\noptuna.visualization.plot_slice(learn, params=[\"lr\"])'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# График истории оптимизации\n",
    "optuna.visualization.plot_optimization_history(learn)\n",
    "\n",
    "# Важность гиперпараметров\n",
    "optuna.visualization.plot_param_importances(learn)\n",
    "\n",
    "# Зависимость loss от lr\n",
    "optuna.visualization.plot_slice(learn, params=[\"lr\"])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d21f2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[149]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m     loss.backward()\n\u001b[32m     14\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     epoch_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(epoch_loss / \u001b[38;5;28mlen\u001b[39m(load_train), epochs)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = CNN(dropout=0.06848746024680974).to(device)\n",
    "model.apply(init_weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=9.920521505714478e-05, weight_decay=0.006146853390949883)\n",
    "for epochs in range(200):\n",
    "    epoch_loss = 0.0 \n",
    "    for images, labels in load_train:\n",
    "        images_cuda = images.to(device)\n",
    "        labels_cuda = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predict = model(images_cuda)\n",
    "        loss = criterion(predict, labels_cuda)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(epoch_loss / len(load_train), epochs) #нечаянно запустил обучения сначала, в первый раз модель обучалась 4 часа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95484a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.75%\n",
      "Accuracy: 85.06%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Отключаем вычисление градиентов (экономия памяти)\n",
    "    for images, labels in load_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Индекс максимального значения\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Отключаем вычисление градиентов (экономия памяти)\n",
    "    for images, labels in load_test:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Индекс максимального значения\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\") #актуальная точность для последнего обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea83ba0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foksm\\OneDrive\\Рабочий стол\\learn_firstly\\ml_eng\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\foksm\\OneDrive\\Рабочий стол\\learn_firstly\\ml_eng\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6249e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[6] = nn.Linear(4096, 18)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier[6].parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18711380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.659635594016627 0\n",
      "2.371012123007523 1\n",
      "2.233970253718527 2\n",
      "2.114411577425505 3\n",
      "2.0739042771489995 4\n",
      "2.0021345684402867 5\n",
      "1.965537829148142 6\n",
      "1.9278491070396022 7\n",
      "1.9011418869620875 8\n",
      "1.8911702984257748 9\n",
      "1.868479081204063 10\n",
      "1.8532165351666903 11\n",
      "1.837268072053006 12\n",
      "1.8288496902114466 13\n",
      "1.8157867990042034 14\n",
      "1.8081754508771395 15\n",
      "1.8032771248566477 16\n",
      "1.7872418416173834 17\n",
      "1.791586608635752 18\n",
      "1.7683535569592526 19\n",
      "1.7701343699505454 20\n",
      "1.7647978450122632 21\n",
      "1.752945015304967 22\n",
      "1.737764483376553 23\n",
      "1.7358796828671506 24\n",
      "1.748011451018484 25\n",
      "1.7384859455259223 26\n",
      "1.7236569530085513 27\n",
      "1.7032738233867444 28\n",
      "1.7112474454076667 29\n",
      "1.715658740620864 30\n",
      "1.7168565367397508 31\n",
      "1.7034600270421882 32\n",
      "1.7103132116167168 33\n",
      "1.6911966311304192 34\n",
      "1.6982233455306606 35\n",
      "1.6719858495812667 36\n",
      "1.6680999228828832 37\n",
      "1.6762843979032416 38\n",
      "1.6959572654021413 39\n",
      "1.6881762805737948 40\n",
      "1.6774518408273396 41\n",
      "1.678440807367626 42\n",
      "1.6741492484745226 43\n",
      "1.674139668439564 44\n",
      "1.6686302448573866 45\n",
      "1.6593846954797444 46\n",
      "1.6736352613097742 47\n",
      "1.6779002798231024 48\n",
      "1.6825621761773761 49\n",
      "1.6375380942696018 50\n",
      "1.6514385273582057 51\n",
      "1.6595890107907747 52\n",
      "1.654262049574601 53\n",
      "1.6423731126283345 54\n",
      "1.6341147134178564 55\n",
      "1.648056063526555 56\n",
      "1.647080661748585 57\n",
      "1.6490412548968667 58\n",
      "1.6418739877249064 59\n",
      "1.6516183175538715 60\n",
      "1.6437004036024998 61\n",
      "1.6336724836575358 62\n",
      "1.6410847243509794 63\n",
      "1.6378639861157065 64\n",
      "1.633703749430807 65\n",
      "1.671561648971156 66\n",
      "1.6198897455867969 67\n",
      "1.6286567929543947 68\n",
      "1.6395597539449993 69\n",
      "1.633759812304848 70\n",
      "1.619248136093742 71\n",
      "1.6417067097990137 72\n",
      "1.603579097358804 73\n",
      "1.6412115548786363 74\n",
      "1.6321196574913828 75\n",
      "1.6253063716386493 76\n",
      "1.630480871075078 77\n",
      "1.6118841528892518 78\n",
      "1.6260000762186553 79\n",
      "1.6152128583506533 80\n",
      "1.6129491263314297 81\n",
      "1.6182525810442472 82\n",
      "1.6189817001945095 83\n",
      "1.6104379716672395 84\n",
      "1.6018332559811441 85\n",
      "1.6102267867640445 86\n",
      "1.6174008247099425 87\n",
      "1.6235317666279643 88\n",
      "1.6221818817289253 89\n",
      "1.608349093010551 90\n",
      "1.623819302885156 91\n",
      "1.5826469578241047 92\n",
      "1.5906682422286587 93\n",
      "1.5928742164059688 94\n",
      "1.6072639860604938 95\n",
      "1.6270596190502769 96\n",
      "1.5981940809049104 97\n",
      "1.629655835503026 98\n",
      "1.6383690200353924 99\n",
      "1.57413313733904 100\n",
      "1.5996765080251192 101\n",
      "1.6166693480391252 102\n",
      "1.6284999659186916 103\n",
      "1.599415525637175 104\n",
      "1.6034640989805522 105\n",
      "1.5872415900230408 106\n",
      "1.6009799605921695 107\n",
      "1.5914959910668824 108\n",
      "1.6071307122707368 109\n",
      "1.586381759141621 110\n",
      "1.5922457547564255 111\n",
      "1.5905163884162903 112\n",
      "1.5947798998732317 113\n",
      "1.6085674207461507 114\n",
      "1.5860767063341643 115\n",
      "1.6150579182725204 116\n",
      "1.6018874306427806 117\n",
      "1.6045147516225513 118\n",
      "1.6027372193963905 119\n",
      "1.603083909499018 120\n",
      "1.6073527725119339 121\n",
      "1.586574827997308 122\n",
      "1.6056452857820611 123\n",
      "1.574808544861643 124\n",
      "1.5842859026632812 125\n",
      "1.5744609744925249 126\n",
      "1.5678533773673209 127\n",
      "1.5806287175730656 128\n",
      "1.5939547300338746 129\n",
      "1.5566957467480709 130\n",
      "1.6029519401098553 131\n",
      "1.5959201116310924 132\n",
      "1.6015525221824647 133\n",
      "1.5965773525990938 134\n",
      "1.6065958659899862 135\n",
      "1.5859951151044744 136\n",
      "1.5762744542799498 137\n",
      "1.5952661250766955 138\n",
      "1.5932000524119327 139\n",
      "1.6158600054289165 140\n",
      "1.589141835664448 141\n",
      "1.6161210279715688 142\n",
      "1.582019952096437 143\n",
      "1.5797236326493715 144\n",
      "1.6071752880748948 145\n",
      "1.5969419460547598 146\n",
      "1.6005442475017748 147\n",
      "1.5910461234418969 148\n",
      "1.5728931088196605 149\n",
      "1.5937113222322965 150\n",
      "1.5878002756520322 151\n",
      "1.588150900288632 152\n",
      "1.5738039923341651 153\n",
      "1.5585033739867964 154\n",
      "1.575054445392207 155\n",
      "1.5657392984942387 156\n",
      "1.5888115663277476 157\n",
      "1.568074547617059 158\n",
      "1.6007923518356524 159\n",
      "1.588901362293645 160\n",
      "1.5904058575630189 161\n",
      "1.5924707989943654 162\n",
      "1.5800071101439626 163\n",
      "1.5548760109826139 164\n",
      "1.5530858689232876 165\n",
      "1.5656291051914817 166\n",
      "1.56875728870693 167\n",
      "1.58558262963044 168\n",
      "1.589137189639242 169\n",
      "1.5965207507735804 170\n",
      "1.547357095542707 171\n",
      "1.573058853651348 172\n",
      "1.5810818885502063 173\n",
      "1.579873658481397 174\n",
      "1.5899128499783968 175\n",
      "1.542617240704988 176\n",
      "1.566286149150447 177\n",
      "1.5706035218740764 178\n",
      "1.5874727123662045 179\n",
      "1.5808216355348887 180\n",
      "1.5470382602591264 181\n",
      "1.5848352523226488 182\n",
      "1.5731590741559078 183\n",
      "1.5761624298597636 184\n",
      "1.5842928974251997 185\n",
      "1.5937068415315527 186\n",
      "1.57993155655108 187\n",
      "1.5986512889987543 188\n",
      "1.5830626258724614 189\n",
      "1.551346135453174 190\n",
      "1.5825149633382496 191\n",
      "1.5991636037826538 192\n",
      "1.5740855643623755 193\n",
      "1.570013291584818 194\n",
      "1.6073489126406217 195\n",
      "1.5965775916450902 196\n",
      "1.5878610152947275 197\n",
      "1.5809670046756141 198\n",
      "1.5817472156725432 199\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(200):\n",
    "    epoch_loss = 0.0 \n",
    "    for images, labels in load_train:\n",
    "        images_cuda = images.to(device)\n",
    "        labels_cuda = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predict = model(images_cuda)\n",
    "        loss = criterion(predict, labels_cuda)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(epoch_loss / len(load_train), epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40a1e544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.49%\n",
      "Accuracy: 46.75%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Отключаем вычисление градиентов (экономия памяти)\n",
    "    for images, labels in load_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Индекс максимального значения\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Отключаем вычисление градиентов (экономия памяти)\n",
    "    for images, labels in load_test:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Индекс максимального значения\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\") #актуальная точность для последнего обучения."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
